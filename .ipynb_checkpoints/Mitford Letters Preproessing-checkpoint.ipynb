{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lxml\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bleach import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'mitford-letters'\n",
    "clean_text_dir = '../mitford_letters_clean'\n",
    "gold_standard_dir = '../mitford_letters_gs'\n",
    "letters = os.listdir(data_dir)\n",
    "letters = [x for x in letters if x not in ['si.xml', '__contents__.xml']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 97 Mitford letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {\n",
    "    'persname': 'person',\n",
    "    'persName': 'person',\n",
    "    'placename': 'place',\n",
    "    'placeName': 'place',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821-05-02-BRHaydon.xml\n",
      "       tag  begin_offset  end_offset           text              ref\n",
      "0   person           115         125     John Keats           #Keats\n",
      "1   person           115         125     John Keats           #Keats\n",
      "2   person           569         580    Mr. Johnson      #Johnson_Mr\n",
      "3   person           639         645         Sister    #Johnson_Miss\n",
      "4   person           569         580    Mr. Johnson      #Johnson_Mr\n",
      "5   person           569         580    Mr. Johnson      #Johnson_Mr\n",
      "6   person           876         889  Mr. Northmore  #Northmore_Thos\n",
      "7   person           876         889  Mr. Northmore  #Northmore_Thos\n",
      "8   person           569         580    Mr. Johnson      #Johnson_Mr\n",
      "9   person          2753        2763     Miss James      #James_Miss\n",
      "10  person          3077        3084        Baldwin       #Baldwin_R\n",
      "11  person          3235        3247   Mr. Talfourd   #Talfourd_Thos\n",
      "12  person          3077        3084        Baldwin       #Baldwin_R\n",
      "13  person          3536        3547    Mr. Baldwin       #Baldwin_R\n",
      "14  person          3847        3857     Mr. Haydon          #Haydon\n",
      "15  person          3886        3890           Papa     #Mitford_Geo\n",
      "16  person          4017        4029   Mr. Macready     #Macready_Wm\n",
      "17  person          4038        4051  Mr. Talfourd>   #Talfourd_Thos\n",
      "18  person          4215        4222        Manager             None\n",
      "19  person          4912        4918         Daphne      #Daphne_pet\n",
      "20  person          5300        5313  M. R. Mitford             #MRM\n",
      "21  person          5326        5337    Mr. Hofland      #Hofland_TC\n",
      "22   place            13          26  Seymour Court       #SeymourCt\n",
      "23   place            32          38         Marlow          #Marlow\n",
      "24   place          1829        1836        Bristol         #Bristol\n",
      "25   place            32          38         Marlow          #Marlow\n",
      "26   place            32          38         Marlow          #Marlow\n",
      "27   place            32          38         Marlow          #Marlow\n",
      "28   place          2285        2291         Thames          #Thames\n",
      "29   place          2403        2415   Bisham Abbey    #Bisham_Abbey\n",
      "30   place          2803        2809         London     #London_city\n",
      "31   place          4524        4531        England         #England\n",
      "32   place          4524        4531        England         #England\n"
     ]
    }
   ],
   "source": [
    "for letter in letters[0:1]:\n",
    "    print(letter)\n",
    "    with open(os.path.join(data_dir, letter), 'r') as f:\n",
    "        content = f.read()\n",
    "    try:\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        soup = soup.findAll('div', {'type': 'letter'})\n",
    "        soup = soup[0]\n",
    "\n",
    "        tags = []\n",
    "        begin_offsets = []\n",
    "        end_offsets = []\n",
    "        tag_text = []\n",
    "        refs = []\n",
    "        past_tags = []\n",
    "        for tag in tag_dict.keys():\n",
    "            for persname in soup.findAll(tag):\n",
    "                pre_count = 0\n",
    "                for t in past_tags:\n",
    "                    if t == persname.text:\n",
    "                        pre_count += 1\n",
    "                begin = soup.text.index(persname.text)\n",
    "                end = len(persname.text) + begin\n",
    "                tags.append(tag_dict[tag])\n",
    "                begin_offsets.append(begin)\n",
    "                end_offsets.append(end)\n",
    "                tag_text.append(persname.text)\n",
    "                past_tags.append(pername.text)\n",
    "                try:\n",
    "                    refs.append(persname['ref'])\n",
    "                except:\n",
    "                    refs.append(None)\n",
    "                #print(persname.text, begin, end)\n",
    "        #for tag in soup.findAll('rs'):\n",
    "        #    begin = soup.text.index(tag.text)\n",
    "        #    end = len(tag.text) + begin\n",
    "        #    tags.append(tag['type'])\n",
    "        #    begin_offsets.append(begin)\n",
    "        #    end_offsets.append(end)\n",
    "        #    tag_text.append(tag.text)\n",
    "        #    try:\n",
    "        #        refs.append(tag['ref'])\n",
    "        #    except:\n",
    "        #        refs.append(None)            \n",
    "        df = pd.DataFrame({'tag': tags,\n",
    "                          'begin_offset': begin_offsets,\n",
    "                          'end_offset': end_offsets,\n",
    "                          'text': tag_text,\n",
    "                          'ref': refs})\n",
    "        #df.drop_duplicates(subset=['begin_offset', 'end_offset', 'text'], inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    df.to_csv(os.path.join(gold_standard_dir, letter), index=False)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821-05-02-BRHaydon.xml\n",
      "1819-08-08-f378-BRHaydon.xml\n",
      "1819-08-22_MossysDeath.xml\n",
      "1823-05-13_Elford.xml\n",
      "1821-11-16-Talfourd.xml\n",
      "1819-12-xa_Webb.xml\n",
      "1819-01-20-MaryWebb.xml\n",
      "1823-04-25-WElford.xml\n",
      "1819-03-02-MWebb.xml\n",
      "1820-11-11-SirWilliamElford.xml\n",
      "1820-11-04Unknown.xml\n",
      "1819-05-30_Elford.xml\n",
      "1819-05-16_MWebb.xml\n",
      "1823-01-13_WElford.xml\n",
      "1821Nov12_13.Talfourd.xml\n",
      "1819-06-29-Elford.xml\n",
      "1822-08-31_Talfourd.xml\n",
      "1818-01-12_WElford.xml\n",
      "1821-04-19-Talfourd.xml\n",
      "1820-07-05_WElford.xml\n",
      "1822-10-12-Elford.xml\n",
      "1821-11-30_Talfourd.xml\n",
      "1820-09-01_Haydon.xml\n",
      "1819-01-10-MaryWebb.xml\n",
      "1820-12-08-MWebb.xml\n",
      "1823-04-09-Hamilton.xml\n",
      "1820-06-29_Haydon.xml\n",
      "1821-10-31-BRHaydon.xml\n",
      "1823-02-28_WElford.xml\n",
      "1819-05-14_Elford.xml\n",
      "1821-01-29_Webb.xml\n",
      "1820-03-20-WElford.xml\n",
      "1822-06-13-BRHaydon.xml\n",
      "1821-03-10-BRHaydon.xml\n",
      "1820-09-30-Elford.xml\n",
      "1821-10-22_Talfourd.xml\n",
      "1820-11-27-SirWilliamElford.xml\n",
      "1820-09-14_BRHaydon.xml\n",
      "1821-02-09-BRHaydon.xml\n",
      "1825-06-29-Talfourd.xml\n",
      "1822-05-19-Talfourd.xml\n",
      "1825-05-11-Talfourd.xml\n",
      "1821-01-07-BRHaydon.xml\n",
      "1819-06-08-Elford.xml\n",
      "1819-03-22_MWebb.xml\n",
      "1823-08-21_Elford.xml\n",
      "1819-11-06_BRHaydon.xml\n",
      "18211122Talfourd.xml\n",
      "1822-03-02-Elford.xml\n",
      "1819-04-19-MWebb.xml\n",
      "1823-04-03-G-Mitford.xml\n",
      "1821-06-21_Talfourd.xml\n",
      "1821-03-22-Elford.xml\n",
      "1823-10-01-Haydon.xml\n",
      "1820-11-04_Haydon.xml\n",
      "1820-08-24_Elford.xml\n",
      "1822-04-28-Spillar.xml\n",
      "1819-01-12_Elford.xml\n",
      "1821_11_23_Elford.xml\n",
      "1819-04-08_Elford.xml\n",
      "1820-10-02-Haydon.xml\n",
      "1820-09-09-Elford.xml\n",
      "1819-02-13_Haydon.xml\n",
      "1819-07-05-MWebb.xml\n",
      "1821-04-04-Elford.xml\n",
      "1821-07-01_Elford.xml\n",
      "1821-07-31-Talfourd.xml\n",
      "1820-01-24-SirWilliamElford.xml\n",
      "1823-03-25_Haydon.xml\n",
      "1819-02-18_Elford.xml\n",
      "1820-02-11-MWebb.xml\n",
      "1819-12-10-f392-MrsMitford.xml\n",
      "1824-06-13-TNTalfourd.xml\n",
      "1819-12-09_MWebb.xml\n",
      "1825-07-29_Talfourd.xml\n",
      "1823-04-26_Macready.xml\n",
      "1821-02-08_Elford.xml\n",
      "1822-08-13-Talfourd.xml\n",
      "1820-12-19-MWebb.xml\n",
      "1819-01-09-Elford.xml\n",
      "1825-01-07_Talfourd.xml\n",
      "1821-11-06-Talfourd.xml\n",
      "1819-03-13_Elford.xml\n",
      "1819-02-10_Elford.xml\n",
      "1819-02-27_Elford.xml\n",
      "1821-10-28-Talfourd.xml\n",
      "1821-04-20_Elford.xml\n",
      "1821-04-18-BRHaydon.xml\n",
      "1820-01-30-Elford.xml\n",
      "1822-07-23-Talfourd.xml\n",
      "1825-08-12Talfourd.xml\n",
      "1819-12-04_Elford.xml\n",
      "1821-11-22-Talfourd.xml\n",
      "1824-02-09_BRHaydon_mjk.xml\n",
      "1823-08-24-Haydon.xml\n",
      "1819-12-23_MWebb.xml\n",
      "1823-06-14_Haydon.xml\n"
     ]
    }
   ],
   "source": [
    "for letter in letters:\n",
    "    print(letter)\n",
    "    with open(os.path.join(data_dir, letter), 'r') as f:\n",
    "        content = f.read()\n",
    "    try:\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        soup = soup.findAll('div', {'type': 'letter'})\n",
    "        soup = soup[0]\n",
    "    \n",
    "        text = soup.text\n",
    "        with open(os.path.join(clean_text_dir, letter), 'w') as f:\n",
    "            f.write(text)\n",
    "    except:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
